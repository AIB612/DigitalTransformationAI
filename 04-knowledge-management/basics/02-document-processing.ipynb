{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æ¡£å¤„ç†ä¸åˆ†å—ç­–ç•¥\n",
    "> By SherryAGI | ä¼ä¸šæ–‡æ¡£çš„ç»“æ„åŒ–å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¼ä¸šå¸¸è§æ–‡æ¡£ç±»å‹\n",
    "\n",
    "| ç±»å‹ | æ ¼å¼ | å¤„ç†éš¾åº¦ |\n",
    "|------|------|----------|\n",
    "| æ–‡æœ¬æ–‡æ¡£ | .txt, .md | â­ |\n",
    "| Office | .docx, .xlsx, .pptx | â­â­ |\n",
    "| PDF | .pdf | â­â­â­ |\n",
    "| ç½‘é¡µ | .html | â­â­ |\n",
    "| æ•°æ®åº“ | SQL | â­â­â­ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "# pip install pypdf python-docx openpyxl beautifulsoup4\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print('ğŸš€ æ–‡æ¡£å¤„ç†æ¨¡å— by SherryAGI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ–‡æ¡£åŠ è½½å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    \"\"\"å¤šæ ¼å¼æ–‡æ¡£åŠ è½½å™¨ by SherryAGI\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_txt(file_path):\n",
    "        \"\"\"åŠ è½½æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdf(file_path):\n",
    "        \"\"\"åŠ è½½ PDF æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            from pypdf import PdfReader\n",
    "            reader = PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "        except ImportError:\n",
    "            return \"è¯·å®‰è£… pypdf: pip install pypdf\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_docx(file_path):\n",
    "        \"\"\"åŠ è½½ Word æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            from docx import Document\n",
    "            doc = Document(file_path)\n",
    "            return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        except ImportError:\n",
    "            return \"è¯·å®‰è£… python-docx: pip install python-docx\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_xlsx(file_path):\n",
    "        \"\"\"åŠ è½½ Excel æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_excel(file_path)\n",
    "            return df.to_string()\n",
    "        except ImportError:\n",
    "            return \"è¯·å®‰è£… openpyxl: pip install openpyxl\"\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, file_path):\n",
    "        \"\"\"è‡ªåŠ¨è¯†åˆ«æ ¼å¼å¹¶åŠ è½½\"\"\"\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        loaders = {\n",
    "            '.txt': cls.load_txt,\n",
    "            '.md': cls.load_txt,\n",
    "            '.pdf': cls.load_pdf,\n",
    "            '.docx': cls.load_docx,\n",
    "            '.xlsx': cls.load_xlsx,\n",
    "        }\n",
    "        loader = loaders.get(ext)\n",
    "        if loader:\n",
    "            return loader(file_path)\n",
    "        return f\"ä¸æ”¯æŒçš„æ ¼å¼: {ext}\"\n",
    "\n",
    "print('âœ… DocumentLoader å·²å®šä¹‰')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åˆ†å—ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": ,
   "outputs": [],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"æ–‡æœ¬åˆ†å—å™¨ by SherryAGI\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fixed_size(text, chunk_size=500, overlap=50):\n",
    "        \"\"\"å›ºå®šå¤§å°åˆ†å—\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunks.append(text[start:end])\n",
    "            start = end - overlap\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def by_sentence(text, max_chunk_size=500):\n",
    "        \"\"\"æŒ‰å¥å­åˆ†å—\"\"\"\n",
    "        import re\n",
    "        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\\s*', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
    "                current_chunk += sentence\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = sentence\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def by_paragraph(text):\n",
    "        \"\"\"æŒ‰æ®µè½åˆ†å—\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        return [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic(text, chunk_size=500):\n",
    "        \"\"\"è¯­ä¹‰åˆ†å—ï¼ˆåŸºäºæ ‡é¢˜/ç« èŠ‚ï¼‰\"\"\"\n",
    "        import re\n",
    "        # æŒ‰æ ‡é¢˜åˆ†å‰²\n",
    "        sections = re.split(r'\\n(?=#{1,3}\\s|\\d+\\.\\s)', text)\n",
    "        \n",
    "        chunks = []\n",
    "        for section in sections:\n",
    "            if len(section) <= chunk_size:\n",
    "                chunks.append(section.strip())\n",
    "            else:\n",
    "                # å¤§æ®µè½å†ç»†åˆ†\n",
    "                sub_chunks = TextChunker.fixed_size(section, chunk_size)\n",
    "                chunks.extend(sub_chunks)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "print('âœ… TextChunker å·²å®šä¹‰')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åˆ†å—ç­–ç•¥å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹æ–‡æ¡£\n",
    "sample_doc = \"\"\"# å‘˜å·¥æ‰‹å†Œ\n",
    "\n",
    "## ç¬¬ä¸€ç«  å…¬å¸ç®€ä»‹\n",
    "\n",
    "æˆ‘ä»¬å…¬å¸æˆç«‹äº2020å¹´ï¼Œä¸“æ³¨äºäººå·¥æ™ºèƒ½æŠ€æœ¯ç ”å‘ã€‚å…¬å¸æ€»éƒ¨ä½äºç‘å£«è‹é»ä¸–ï¼Œåœ¨å…¨çƒæ‹¥æœ‰è¶…è¿‡500åå‘˜å·¥ã€‚æˆ‘ä»¬çš„ä½¿å‘½æ˜¯ç”¨AIæŠ€æœ¯æ”¹å˜ä¸–ç•Œã€‚\n",
    "\n",
    "## ç¬¬äºŒç«  å…¥èŒæµç¨‹\n",
    "\n",
    "æ–°å‘˜å·¥å…¥èŒéœ€è¦å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "1. ç­¾ç½²åŠ³åŠ¨åˆåŒ\n",
    "2. æäº¤èº«ä»½è¯æ˜æ–‡ä»¶\n",
    "3. å‚åŠ å…¥èŒåŸ¹è®­\n",
    "4. é¢†å–åŠå…¬è®¾å¤‡\n",
    "\n",
    "å…¥èŒåŸ¹è®­ä¸ºæœŸä¸‰å¤©ï¼ŒåŒ…æ‹¬å…¬å¸æ–‡åŒ–ã€äº§å“ä»‹ç»ã€å®‰å…¨åŸ¹è®­ç­‰å†…å®¹ã€‚\n",
    "\n",
    "## ç¬¬ä¸‰ç«  è–ªé…¬ç¦åˆ©\n",
    "\n",
    "å…¬å¸æä¾›æœ‰ç«äº‰åŠ›çš„è–ªé…¬å¾…é‡ï¼ŒåŒ…æ‹¬ï¼š\n",
    "- åŸºæœ¬å·¥èµ„\n",
    "- ç»©æ•ˆå¥–é‡‘\n",
    "- å¹´ç»ˆå¥–\n",
    "- è‚¡ç¥¨æœŸæƒ\n",
    "\n",
    "ç¦åˆ©åŒ…æ‹¬ï¼šäº”é™©ä¸€é‡‘ã€è¡¥å……åŒ»ç–—ä¿é™©ã€å¹´åº¦ä½“æ£€ã€å¸¦è–ªå¹´å‡ç­‰ã€‚\n",
    "\"\"\"\n",
    "\n",
    "print('ğŸ“„ åŸæ–‡æ¡£é•¿åº¦:', len(sample_doc), 'å­—ç¬¦')\n",
    "print('\\n--- ä¸åŒåˆ†å—ç­–ç•¥å¯¹æ¯” ---\\n')\n",
    "\n",
    "# å›ºå®šå¤§å°\n",
    "fixed_chunks = TextChunker.fixed_size(sample_doc, 200, 30)\n",
    "print(f'å›ºå®šå¤§å°åˆ†å—: {len(fixed_chunks)} å—')\n",
    "\n",
    "# æŒ‰æ®µè½\n",
    "para_chunks = TextChunker.by_paragraph(sample_doc)\n",
    "print(f'æŒ‰æ®µè½åˆ†å—: {len(para_chunks)} å—')\n",
    "\n",
    "# è¯­ä¹‰åˆ†å—\n",
    "semantic_chunks = TextChunker.semantic(sample_doc, 300)\n",
    "print(f'è¯­ä¹‰åˆ†å—: {len(semantic_chunks)} å—')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹è¯­ä¹‰åˆ†å—ç»“æœ\n",
    "print('ğŸ“ è¯­ä¹‰åˆ†å—ç»“æœ:\\n')\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f'--- å— {i+1} ({len(chunk)} å­—ç¬¦) ---')\n",
    "    print(chunk[:150] + '...' if len(chunk) > 150 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å…ƒæ•°æ®æå–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataExtractor:\n",
    "    \"\"\"å…ƒæ•°æ®æå–å™¨ by SherryAGI\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_path(file_path):\n",
    "        \"\"\"ä»æ–‡ä»¶è·¯å¾„æå–å…ƒæ•°æ®\"\"\"\n",
    "        path = Path(file_path)\n",
    "        return {\n",
    "            \"filename\": path.name,\n",
    "            \"extension\": path.suffix,\n",
    "            \"directory\": str(path.parent),\n",
    "            \"size_bytes\": path.stat().st_size if path.exists() else 0\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_content(text):\n",
    "        \"\"\"ä»å†…å®¹æå–å…ƒæ•°æ®\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # æå–æ ‡é¢˜\n",
    "        title_match = re.search(r'^#\\s+(.+)$', text, re.MULTILINE)\n",
    "        title = title_match.group(1) if title_match else None\n",
    "        \n",
    "        # æå–ç« èŠ‚\n",
    "        sections = re.findall(r'^##\\s+(.+)$', text, re.MULTILINE)\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"sections\": sections,\n",
    "            \"char_count\": len(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"line_count\": len(text.split('\\n'))\n",
    "        }\n",
    "\n",
    "# æµ‹è¯•\n",
    "metadata = MetadataExtractor.extract_from_content(sample_doc)\n",
    "print('ğŸ“‹ æå–çš„å…ƒæ•°æ®:')\n",
    "for k, v in metadata.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å®Œæ•´å¤„ç†æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"æ–‡æ¡£å¤„ç†å™¨ by SherryAGI\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=500, chunk_overlap=50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def process(self, file_path=None, text=None, metadata=None):\n",
    "        \"\"\"å¤„ç†æ–‡æ¡£ï¼Œè¿”å›å¸¦å…ƒæ•°æ®çš„åˆ†å—\"\"\"\n",
    "        \n",
    "        # 1. åŠ è½½æ–‡æ¡£\n",
    "        if file_path:\n",
    "            content = DocumentLoader.load(file_path)\n",
    "            file_metadata = MetadataExtractor.extract_from_path(file_path)\n",
    "        else:\n",
    "            content = text\n",
    "            file_metadata = {}\n",
    "        \n",
    "        # 2. æå–å†…å®¹å…ƒæ•°æ®\n",
    "        content_metadata = MetadataExtractor.extract_from_content(content)\n",
    "        \n",
    "        # 3. åˆ†å—\n",
    "        chunks = TextChunker.semantic(content, self.chunk_size)\n",
    "        \n",
    "        # 4. ç»„è£…ç»“æœ\n",
    "        results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            results.append({\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    **file_metadata,\n",
    "                    **content_metadata,\n",
    "                    **(metadata or {}),\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "processor = DocumentProcessor(chunk_size=300)\n",
    "processed = processor.process(text=sample_doc, metadata={\"source\": \"å‘˜å·¥æ‰‹å†Œ\", \"version\": \"2026\"})\n",
    "\n",
    "print(f'âœ… å¤„ç†å®Œæˆï¼Œå…± {len(processed)} ä¸ªåˆ†å—')\n",
    "print(f'\\nç¬¬ä¸€ä¸ªåˆ†å—çš„å…ƒæ•°æ®:')\n",
    "for k, v in processed[0][\"metadata\"].items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æœ€ä½³å®è·µ\n",
    "\n",
    "| æ–‡æ¡£ç±»å‹ | æ¨èåˆ†å—ç­–ç•¥ | å—å¤§å° |\n",
    "|----------|--------------|--------|\n",
    "| æŠ€æœ¯æ–‡æ¡£ | è¯­ä¹‰åˆ†å—ï¼ˆæŒ‰ç« èŠ‚ï¼‰ | 500-1000 |\n",
    "| æ”¿ç­–æ–‡ä»¶ | æŒ‰æ®µè½ | 300-500 |\n",
    "| FAQ | æŒ‰é—®ç­”å¯¹ | 200-400 |\n",
    "| ä»£ç  | æŒ‰å‡½æ•°/ç±» | 500-800 |\n",
    "| èŠå¤©è®°å½• | æŒ‰å¯¹è¯è½®æ¬¡ | 300-500 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**By SherryAGI** | [DigitalTransformationAI](https://github.com/AIB612/DigitalTransformationAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
