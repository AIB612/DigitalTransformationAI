{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG åŸºç¡€æ¶æ„\n",
    "> By SherryAGI | Retrieval-Augmented Generation å…¥é—¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä»€ä¹ˆæ˜¯ RAGï¼Ÿ\n",
    "\n",
    "**Retrieval-Augmented Generation** = æ£€ç´¢å¢å¼ºç”Ÿæˆ\n",
    "\n",
    "```\n",
    "ç”¨æˆ·æé—® â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ ç»“åˆæ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ\n",
    "```\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦ RAGï¼Ÿ\n",
    "\n",
    "| é—®é¢˜ | çº¯ LLM | RAG |\n",
    "|------|--------|-----|\n",
    "| çŸ¥è¯†è¿‡æ—¶ | âŒ è®­ç»ƒæ•°æ®æˆªæ­¢ | âœ… å®æ—¶æ£€ç´¢æœ€æ–°æ–‡æ¡£ |\n",
    "| ç§æœ‰æ•°æ® | âŒ æ— æ³•è®¿é—® | âœ… è¿æ¥ä¼ä¸šçŸ¥è¯†åº“ |\n",
    "| å¹»è§‰é—®é¢˜ | âŒ å¯èƒ½ç¼–é€  | âœ… åŸºäºçœŸå®æ–‡æ¡£ |\n",
    "| æ¥æºè¿½æº¯ | âŒ æ— æ³•éªŒè¯ | âœ… å¼•ç”¨åŸæ–‡å‡ºå¤„ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "# pip install langchain langchain-openai chromadb\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "print('ğŸš€ RAG åŸºç¡€æ¶æ„ by SherryAGI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG æ ¸å¿ƒæµç¨‹\n",
    "\n",
    "```\n",
    "1. æ–‡æ¡£åŠ è½½ (Load)\n",
    "   â†“\n",
    "2. æ–‡æœ¬åˆ†å— (Split)\n",
    "   â†“\n",
    "3. å‘é‡åŒ– (Embed)\n",
    "   â†“\n",
    "4. å­˜å‚¨ (Store)\n",
    "   â†“\n",
    "5. æ£€ç´¢ (Retrieve)\n",
    "   â†“\n",
    "6. ç”Ÿæˆ (Generate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ–‡æ¡£åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹ï¼šä¼ä¸šçŸ¥è¯†æ–‡æ¡£\n",
    "documents = [\n",
    "    {\n",
    "        \"content\": \"\"\"å…¬å¸æŠ¥é”€æ”¿ç­–ï¼š\n",
    "        1. å·®æ—…è´¹ï¼šç»æµèˆ±æœºç¥¨ã€ä¸‰æ˜Ÿçº§é…’åº—\n",
    "        2. é¤é¥®è´¹ï¼šæ¯æ—¥ä¸Šé™ 100 å…ƒ\n",
    "        3. äº¤é€šè´¹ï¼šå…¬å…±äº¤é€šæˆ–æ‰“è½¦ï¼ˆéœ€å‘ç¥¨ï¼‰\n",
    "        4. å®¡æ‰¹æµç¨‹ï¼š500å…ƒä»¥ä¸‹ç›´æ¥æŠ¥é”€ï¼Œ500å…ƒä»¥ä¸Šéœ€ä¸»ç®¡å®¡æ‰¹\"\"\",\n",
    "        \"metadata\": {\"source\": \"HRæ”¿ç­–\", \"category\": \"æŠ¥é”€\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"è¯·å‡åˆ¶åº¦ï¼š\n",
    "        1. å¹´å‡ï¼šå…¥èŒæ»¡1å¹´äº«æœ‰5å¤©ï¼Œæ¯å¢åŠ 1å¹´åŠ 1å¤©ï¼Œä¸Šé™15å¤©\n",
    "        2. ç—…å‡ï¼šéœ€æä¾›åŒ»é™¢è¯æ˜ï¼Œæ¯å¹´ç´¯è®¡ä¸è¶…è¿‡30å¤©\n",
    "        3. äº‹å‡ï¼šéœ€æå‰3å¤©ç”³è¯·ï¼Œæ¯å¹´ç´¯è®¡ä¸è¶…è¿‡10å¤©\n",
    "        4. å®¡æ‰¹ï¼š3å¤©ä»¥å†…ä¸»ç®¡å®¡æ‰¹ï¼Œ3å¤©ä»¥ä¸Šéœ€HRå®¡æ‰¹\"\"\",\n",
    "        \"metadata\": {\"source\": \"HRæ”¿ç­–\", \"category\": \"è¯·å‡\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"IT æ”¯æŒæµç¨‹ï¼š\n",
    "        1. æäº¤å·¥å•ï¼šé€šè¿‡å†…éƒ¨ç³»ç»Ÿæäº¤\n",
    "        2. å“åº”æ—¶é—´ï¼šæ™®é€šé—®é¢˜24å°æ—¶ï¼Œç´§æ€¥é—®é¢˜4å°æ—¶\n",
    "        3. è®¾å¤‡ç”³è¯·ï¼šéœ€éƒ¨é—¨ä¸»ç®¡å®¡æ‰¹\n",
    "        4. å¯†ç é‡ç½®ï¼šå¯è‡ªåŠ©æˆ–è”ç³» IT çƒ­çº¿ 8888\"\"\",\n",
    "        \"metadata\": {\"source\": \"ITæ‰‹å†Œ\", \"category\": \"æ”¯æŒ\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f'åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ–‡æœ¬åˆ†å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†å—ç­–ç•¥\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,      # æ¯å—æœ€å¤§å­—ç¬¦æ•°\n",
    "    chunk_overlap=50,    # å—ä¹‹é—´é‡å å­—ç¬¦æ•°\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼Œ\", \" \"]\n",
    ")\n",
    "\n",
    "# åˆ†å—\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    splits = text_splitter.split_text(doc[\"content\"])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"content\": split,\n",
    "            \"metadata\": doc[\"metadata\"]\n",
    "        })\n",
    "\n",
    "print(f'åˆ†å—åå…± {len(chunks)} ä¸ªç‰‡æ®µ')\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f'\\n--- ç‰‡æ®µ {i+1} ---')\n",
    "    print(chunk[\"content\"][:100] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å‘é‡åŒ– & å­˜å‚¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ OpenAI Embeddingsï¼ˆéœ€è¦ API Keyï¼‰\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "# æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆæ¨èä¼ä¸šä½¿ç”¨ï¼‰\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
    "\n",
    "# ç¤ºä¾‹ï¼šæ¨¡æ‹Ÿå‘é‡åŒ–\n",
    "import numpy as np\n",
    "\n",
    "def mock_embed(text):\n",
    "    \"\"\"æ¨¡æ‹Ÿå‘é‡åŒ–ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå® Embedding æ¨¡å‹ï¼‰\"\"\"\n",
    "    np.random.seed(hash(text) % 2**32)\n",
    "    return np.random.rand(384).tolist()\n",
    "\n",
    "# å‘é‡åŒ–æ‰€æœ‰ç‰‡æ®µ\n",
    "for chunk in chunks:\n",
    "    chunk[\"embedding\"] = mock_embed(chunk[\"content\"])\n",
    "\n",
    "print(f'å‘é‡ç»´åº¦: {len(chunks[0][\"embedding\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ£€ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search(query, top_k=3):\n",
    "    \"\"\"è¯­ä¹‰æœç´¢\"\"\"\n",
    "    query_embedding = mock_embed(query)\n",
    "    \n",
    "    # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "    similarities = []\n",
    "    for chunk in chunks:\n",
    "        sim = cosine_similarity([query_embedding], [chunk[\"embedding\"]])[0][0]\n",
    "        similarities.append((sim, chunk))\n",
    "    \n",
    "    # æ’åºè¿”å› Top K\n",
    "    similarities.sort(key=lambda x: -x[0])\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# æµ‹è¯•æ£€ç´¢\n",
    "query = \"å‡ºå·®æŠ¥é”€æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "results = search(query)\n",
    "\n",
    "print(f'ğŸ” æŸ¥è¯¢: {query}\\n')\n",
    "for i, (score, chunk) in enumerate(results):\n",
    "    print(f'--- ç»“æœ {i+1} (ç›¸ä¼¼åº¦: {score:.3f}) ---')\n",
    "    print(f'æ¥æº: {chunk[\"metadata\"][\"source\"]}')\n",
    "    print(chunk[\"content\"][:150])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ç”Ÿæˆç­”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context_chunks):\n",
    "    \"\"\"åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆï¼ˆç¤ºä¾‹ï¼‰\"\"\"\n",
    "    \n",
    "    # æ„å»º Prompt\n",
    "    context = \"\\n\\n\".join([c[\"content\"] for _, c in context_chunks])\n",
    "    \n",
    "    prompt = f\"\"\"åŸºäºä»¥ä¸‹ä¼ä¸šçŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»å†…å®¹ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯´æ˜ã€‚\n",
    "\n",
    "çŸ¥è¯†åº“å†…å®¹ï¼š\n",
    "{context}\n",
    "\n",
    "é—®é¢˜ï¼š{query}\n",
    "\n",
    "ç­”æ¡ˆï¼š\"\"\"\n",
    "    \n",
    "    # å®é™…ä½¿ç”¨æ—¶è°ƒç”¨ LLM\n",
    "    # response = llm.invoke(prompt)\n",
    "    \n",
    "    print('ğŸ“ ç”Ÿæˆçš„ Prompt:')\n",
    "    print(prompt)\n",
    "    print('\\nğŸ’¡ æç¤ºï¼šå°†æ­¤ Prompt å‘é€ç»™ LLM å³å¯è·å¾—ç­”æ¡ˆ')\n",
    "\n",
    "generate_answer(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å®Œæ•´ RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"ç®€å• RAG å®ç° by SherryAGI\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunks = []\n",
    "    \n",
    "    def add_document(self, content, metadata=None):\n",
    "        \"\"\"æ·»åŠ æ–‡æ¡£\"\"\"\n",
    "        chunk = {\n",
    "            \"content\": content,\n",
    "            \"metadata\": metadata or {},\n",
    "            \"embedding\": mock_embed(content)\n",
    "        }\n",
    "        self.chunks.append(chunk)\n",
    "    \n",
    "    def query(self, question, top_k=3):\n",
    "        \"\"\"æŸ¥è¯¢\"\"\"\n",
    "        results = search(question, top_k)\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"sources\": [c[\"metadata\"].get(\"source\", \"æœªçŸ¥\") for _, c in results],\n",
    "            \"context\": [c[\"content\"] for _, c in results]\n",
    "        }\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "rag = SimpleRAG()\n",
    "for doc in documents:\n",
    "    rag.add_document(doc[\"content\"], doc[\"metadata\"])\n",
    "\n",
    "result = rag.query(\"å¹´å‡æœ‰å¤šå°‘å¤©ï¼Ÿ\")\n",
    "print(f'é—®é¢˜: {result[\"question\"]}')\n",
    "print(f'æ¥æº: {result[\"sources\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**By SherryAGI** | [DigitalTrans](https://github.com/AIB612/DigitalTrans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
